# Transformer/Attention 手動実装プロジェクト

## プロジェクト概要

日本語手話通訳アプリの開発に向けた、TransformerとAttentionメカニズムの学習プロジェクト。

**現在**: Phase 15b（Seq2Seq翻訳モデル）
**プロジェクト**: `transformer_burn`（Burn 0.18.0使用）

## 技術スタック

- **言語**: Rust
- **フレームワーク**: Burn 0.18.0
- **バックエンド**: Wgpu（GPU）、Autodiff（自動微分）
- **最適化**: Adam

## 現在のモデル設定

- **語彙サイズ**: 168（日本語86 + 手話タグ80 + PAD + SOS/EOS）
- **シーケンス長**: 10
- **モデル次元**: d_model=16、2ヘッド（d_head=8）、d_ff=32
- **層数**: 4層Transformerブロック
- **アーキテクチャ**: Pre-LN方式、残差接続

---

## Phase 15b: Seq2Seq翻訳モデル（進行中）

**目標**: 日本語→手話タグへの可変長翻訳（Encoder-Decoder）

**実装内容**:

- Encoder層（Self-Attention + FF）
- Cross-Attention層
- Decoder拡張（Self + Cross-Attention + FF）
- 可変長出力（SOS/EOSトークン、自己回帰生成）

### 実装予定

- [x] より深いモデル（3-4層）への拡張
- [x] 可変長シーケンス（パディングとマスキング）
- [x] モデル次元の拡大（d_model=16）
- [x] Burn移行（バッチ処理で8.6倍、GPU最適化で15.5倍高速化達成）
- [x] **Phase 15a**: 日本語→手話タグ翻訳（デコーダーのみ、固定5タグ出力）
- [ ] **Phase 15b**: Seq2Seq翻訳モデル（エンコーダー・デコーダー、可変長出力）← 現在ここ
- [ ] より長いシーケンス（20-50文字）への拡張
- [ ] 大規模モデル（d_model=64-256）への拡張

---

## ユーザー背景

- Next.js、TypeScript、Tailwind CSS、Supabaseでの開発経験
- **Rust初心者**: 基本文法は理解しているが、細かい仕様は学習中
- UI/UX設計、社会的価値、ケア・ステークホルダー視点を重視

---

## コミュニケーション指針（重要）

### YOU MUST follow these communication rules:

1. **言語**: 必ず日本語で回答すること

2. **スタイル**:
   - 論理的で簡潔、かつ実践的で実行可能
   - 段階的な説明を重視
   - 形式的だが過度に堅苦しくない
   - 絵文字を使わないこと

3. **コード指示の出し方**（CRITICAL）:
   - **必ず行数を明示する**: 「XXX行目に追加」「YYY行目を修正」と具体的に指定
   - **Rust初心者を考慮**: タイプ、所有権、トレイトなど、分かりにくい部分は丁寧に説明
   - **修正前後を明示**: 変更箇所が明確に分かるように提示
   - **複数ファイルの場合**: ファイル名と行数を両方明記

4. **不確実性**: 分からないことや限界は率直に認める

5. **絵文字**: 使用しない

### 指示の例

```
src/attention.rs の 215行目の後に追加:

修正前（103行目）:
pub fn create_output_weight() -> Matrix {

修正後:
fn create_output_weight() -> Matrix {
```

### 対話方針

- アルゴリズムの考え方、実装方針、デバッグのアプローチを説明
- 具体的なコードではなく、疑似コードや概念図で説明
- 行き詰まった際の問題解決をサポート
- 建設的で論理的なフィードバックを提供
- Rust特有の概念（借用、ライフタイム、トレイト）は必要に応じて補足説明

---

**Note**: このファイルはリビングドキュメントとして、実装の進捗に応じて更新する。
**最終更新**: Phase 15b開始（2025年10月19日）
