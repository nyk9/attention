# Transformer/Attention 手動実装プロジェクト

## プロジェクト概要

日本語手話通訳アプリの開発に向けた基礎学習として、TransformerとAttentionメカニズムを完全手動実装（外部ライブラリなし）。

**完了フェーズ**: Phase 1-13（4層Transformer + 可変長入力対応）
**進行中フェーズ**: Phase 14（Burn移行、GPU活用）

## 現在のステータス（Phase 14進行中）

### Phase 14: Burn移行プロジェクト（進行中）

**プロジェクト名**: `transformer_burn`
**場所**: `rust/attention/transformer_burn/`
**開始日**: 2025年10月10日

#### 完了した実装

✅ **インフラ・基盤**:
- Cargo.toml設定（Burn 0.18.0 + wgpu backend）
- モジュール構造（config, vocabulary, model, data）
- 訓練データ移行（2879サンプル生成）

✅ **モデル層**:
- Embedding層（`burn::nn::Embedding` + カスタムPositional Encoding）
- Multi-Head Attention（カスタム実装、2ヘッド、d_head=8）
- Feed-Forward層（`burn::nn::Linear` + ReLU、d_ff=32）
- Layer Normalization（`burn::nn::LayerNorm`）
- Transformerブロック（Pre-LN方式、残差接続×2）
- 4層スタック構造（NUM_LAYERS=4）
- 出力射影層（d_model → vocab_size=100）

✅ **データ層**:
- Vocabulary実装（既存ロジック移植、100語彙）
- TrainingDataローダー（training_data.txt読み込み、2879サンプル）

⏳ **訓練・推論層（実装中）**:
- [ ] 訓練ループ（Autodiff + Adam最適化）
- [ ] 損失関数（CrossEntropyLoss）
- [ ] テキスト生成機能

#### 技術スタック（Phase 14）

- **フレームワーク**: Burn 0.18.0
- **バックエンド**: Wgpu（GPU対応）
- **自動微分**: Autodiff
- **最適化**: Adam
- **モデル設定**: Phase 13と同一（d_model=16, 4層, 2ヘッド）

#### 動作確認結果

- **フォワードパス**: 正常動作
- **出力形状**: `[batch=1, seq_len=10, vocab=100]` ✓
- **実行時間**: 約0.5秒（フォワードパスのみ、未最適化）

---

## Phase 13のステータス（完了）

- **アーキテクチャ**: 4層Transformerデコーダー（Pre-LN方式）
- **モデルサイズ**: d_model=16, 2ヘッド, d_ff=32
- **次文字予測精度**: 88.7%（6文字入力）
- **損失**: 0.0100（初期5.19から99.8%改善）
- **訓練時間**: 1675秒（約28分、1000エポック）
- **可変長入力**: 任意長→10文字に自動パディング対応

### 実装予定要素（Phase 14以降）

- [x] より深いモデル（3-4層）への拡張
- [x] 可変長シーケンス（パディングとマスキング）
- [x] モデル次元の拡大（d_model=16）
- [ ] より長いシーケンス（20-50文字）への拡張
- [ ] バイアス項の追加
- [ ] Burn移行（GPU活用で計算速度10-100倍向上を期待）

## 技術制約と実装方針

### 実装方針

#### Phase 1-13（attention_test）

**IMPORTANT**: 以下の実装方針を厳守すること

1. **完全手動実装**: ndarrayなどの外部ライブラリは一切使用しない
2. **段階的アプローチ**: 各層を独立して実装し、段階的に統合
3. **検証重視**: 各層の勾配を数値微分と比較して正確性を確認

#### Phase 14（transformer_burn）

**IMPORTANT**: Burnフレームワーク移行の方針

1. **既存ロジック参考**: attention_testの実装を参考に、Tensor演算に変換
2. **段階的移行**: 層ごとに実装して動作確認（Embedding → Attention → FF → Block）
3. **自動微分活用**: Burnの`Autodiff`で手動バックプロパゲーション不要
4. **GPU最適化**: wgpuバックエンドで10-100倍の高速化を目指す

### 技術スタック

#### Phase 1-13（attention_test）
- **言語**: Rust
- **ライブラリ**: なし（Vec、配列、標準ライブラリのみ）
- **実装レベル**: 行列演算を含む全てを手動実装

#### Phase 14（transformer_burn）
- **言語**: Rust
- **フレームワーク**: Burn 0.18.0
- **バックエンド**: Wgpu（GPU）、Autodiff（自動微分）
- **実装レベル**: Burnの高レベルAPIを活用

### 現在の設定（Phase 13時点）

- 語彙サイズ: 100文字（ひらがな全種類 + 記号 + 長音 + PADトークン）
- シーケンス長: 10文字（SEQ_LEN定数で管理、可変長入力に自動対応）
- モデル次元: **d_model=16**（Phase 12から2倍に拡張）
- Multi-Head: 2ヘッド（d_head=8）
- Feed-Forward中間層: d_ff=32
- **Transformerブロック層数: 4層**（NUM_LAYERS定数で管理）
- **学習パラメータ数: 約27,000個**（各層に独立したパラメータ、埋め込みと出力層は共通）
- 訓練データ: 1378パターン（data/training_data.txt、長文25件追加）
- アーキテクチャ: Pre-LN方式、2箇所の残差接続、複数層スタック構造、Attention Mask
- ハイパーパラメータ: learning_rate=0.0005, epochs=1000

## ユーザー背景

- Next.js、TypeScript、Tailwind CSS、Supabaseでの開発経験
- **Rust初心者**: 基本文法は理解しているが、細かい仕様は学習中
- UI/UX設計、社会的価値、ケア・ステークホルダー視点を重視

### 現在の理解レベル

#### Phase 13完了時点
- ✓ Attention機構、Multi-Head Attention、バックプロパゲーションの完全理解
- ✓ Transformerブロック（Attention + Feed-Forward + Layer Norm + 残差接続）
- ✓ 文字レベル自然言語処理、テキスト生成
- ✓ 複数層のスタック構造、逆伝播の連鎖
- ✓ Pre-LN方式、残差接続の威力、深いモデルの表現力
- ✓ パディング・マスキング機構、可変長シーケンス対応
- ✓ モデル容量とデータ複雑性のバランス、ハイパーパラメータ調整

#### Phase 14進行中（追加習得事項）
- ✓ Burnフレームワークの基本構造（Module, Backend, Tensor）
- ✓ ジェネリクスによるバックエンド抽象化（`<B: Backend>`）
- ✓ Tensorの形状管理とreshape/slice操作
- ✓ Burnの組み込みレイヤー活用（Embedding, Linear, LayerNorm）
- ✓ カスタムモジュール実装（`#[derive(Module)]`マクロ）
- ⏳ Autodiffバックエンドと自動微分（学習中）
- ⏳ Optimizerとパラメータ更新（学習中）

## コミュニケーション指針

### YOU MUST follow these communication rules:

1. **言語**: 必ず日本語で回答すること

2. **スタイル**:
   - 論理的で簡潔、かつ実践的で実行可能
   - 段階的な説明を重視
   - 形式的だが過度に堅苦しくない

3. **コード指示の出し方**（CRITICAL）:
   - **必ず行数を明示する**: 「XXX行目に追加」「YYY行目を修正」と具体的に指定
   - **Rust初心者を考慮**: タイプ、所有権、トレイトなど、分かりにくい部分は丁寧に説明
   - **修正前後を明示**: 変更箇所が明確に分かるように提示
   - **複数ファイルの場合**: ファイル名と行数を両方明記

   例：

   ```
   src/attention.rs の 215行目の後に追加:

   修正前（103行目）:
   pub fn create_output_weight() -> Matrix {

   修正後:
   fn create_output_weight() -> Matrix {
   ```

4. **コード出力**: ソースコードは出力しない（ユーザーが手書きで実装）

5. **不確実性**: 分からないことや限界は率直に認める

6. **絵文字**: 使用しない

### 対話方針

- アルゴリズムの考え方、実装方針、デバッグのアプローチを説明
- 具体的なコードではなく、疑似コードや概念図で説明
- 行き詰まった際の問題解決をサポート
- 建設的で論理的なフィードバックを提供
- Rust特有の概念（借用、ライフタイム、トレイト）は必要に応じて補足説明

## デバッグ支援

### デバッグ時のアプローチ

**基本戦略**:

1. 中間結果を段階的に出力して確認
2. 小さな入力（2x2行列など）で手計算と比較
3. 各関数を独立してテスト
4. エッジケース（ゼロ行列、単位行列）で動作確認

**学習時の確認項目**:

- 数値微分と解析的勾配を比較（勾配チェック）
- 学習曲線を観察（損失が下がっているか）
- 勾配の符号を確認（正解方向に更新されているか）
- 各層の勾配の大きさのバランス

---

**Note**: このファイルはリビングドキュメントとして、実装の進捗に応じて更新する。

**最終更新**: Phase 14進行中（2025年10月10日）
- Phase 14: Burn移行プロジェクト開始
- モデル層・データ層の実装完了
- 訓練ループ実装中
