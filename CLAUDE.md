# Transformer/Attention 手動実装プロジェクト

## プロジェクト概要

日本語手話通訳アプリの開発に向けた基礎学習として、TransformerとAttentionメカニズムを完全手動実装（外部ライブラリなし）。

**完了フェーズ**: Phase 1-14（Burn移行、GPU + バッチ処理対応）
**次フェーズ**: Phase 15（長いシーケンス、大規模モデルへの拡張）

## 現在のステータス（Phase 14完了）

### Phase 14: Burn移行プロジェクト（完了）

**プロジェクト名**: `transformer_burn`
**場所**: `rust/attention/transformer_burn/`
**開始日**: 2025年10月10日

#### 完了した実装

✅ **インフラ・基盤**:

- Cargo.toml設定（Burn 0.18.0 + wgpu backend）
- モジュール構造（config, vocabulary, model, data）
- 訓練データ移行（2879サンプル生成）

✅ **モデル層**:

- Embedding層（`burn::nn::Embedding` + カスタムPositional Encoding）
- Multi-Head Attention（カスタム実装、2ヘッド、d_head=8）
- Feed-Forward層（`burn::nn::Linear` + ReLU、d_ff=32）
- Layer Normalization（`burn::nn::LayerNorm`）
- Transformerブロック（Pre-LN方式、残差接続×2）
- 4層スタック構造（NUM_LAYERS=4）
- 出力射影層（d_model → vocab_size=100）

✅ **データ層**:

- Vocabulary実装（既存ロジック移植、100語彙）
- TrainingDataローダー（training_data.txt読み込み、2879サンプル）
- バッチイテレーター実装（可変バッチサイズ対応）

✅ **訓練・推論層**:

- 訓練ループ（Autodiff + Adam最適化）
- 損失関数（CrossEntropyLoss、パディングトークン除外）
- テキスト生成機能（greedy sampling）
- バッチ処理対応（バッチサイズ32/128で動作確認）

✅ **GPU最適化**:

- Wgpuバックエンド統合（再帰制限256で解決）
- Windows RTX 3070（Vulkan）動作確認
- バッチサイズ最適化（128で最高性能）

#### 技術スタック（Phase 14）

- **フレームワーク**: Burn 0.18.0
- **バックエンド**: Wgpu（GPU、Metal/Vulkan）、NdArray（CPU、比較用）
- **自動微分**: Autodiff
- **最適化**: Adam（beta1=0.9, beta2=0.999, epsilon=1e-8）
- **モデル設定**: Phase 13と同一（d_model=16, 4層, 2ヘッド）
- **バッチサイズ**: 32/128（GPU最適化）

#### 性能比較結果（10エポック）

| バックエンド                    | バッチサイズ      | 実行時間 | 対Phase 13比   |
| ------------------------------- | ----------------- | -------- | -------------- |
| Phase 13 CPU手動実装            | 1（サンプルごと） | 155秒    | 基準           |
| **NdArray (CPU)**               | 32                | **18秒** | **8.6倍高速**  |
| Wgpu (GPU) Windows RTX 3070     | 32                | 37秒     | 4.2倍高速      |
| **Wgpu (GPU) Windows RTX 3070** | **128**           | **10秒** | **15.5倍高速** |

**重要な知見**:

- バッチ処理により8.6倍高速化（CPU）
- GPUはバッチサイズ128で真価発揮（バッチ32では転送オーバーヘッドが支配的）
- 小規模モデルではCPUバッチ処理が最もバランス良い選択肢

#### 訓練結果（100エポック、Windows RTX 3070、バッチ128）

- **訓練時間**: 69秒（Phase 13の1000エポック = 1675秒に対し、100エポックで約1/24の時間）
- **損失**: 4.45 → 2.81
- **生成テキスト**: "こんにちは" → "こんにちはいききます。。。。。。。"
- **学習状況**: 単語境界を学習開始（「いく」「ます」のパターン認識）
- **今後**: 1000エポック訓練で Phase 13 同等の精度（損失0.01、88.7%精度）を目指す

---

## Phase 13のステータス（完了）

- **アーキテクチャ**: 4層Transformerデコーダー（Pre-LN方式）
- **モデルサイズ**: d_model=16, 2ヘッド, d_ff=32
- **次文字予測精度**: 88.7%（6文字入力）
- **損失**: 0.0100（初期5.19から99.8%改善）
- **訓練時間**: 1675秒（約28分、1000エポック）
- **可変長入力**: 任意長→10文字に自動パディング対応

## Phase 15: 日本語→手話タグ翻訳システム（計画）

**目標**: 日本語テキスト → 意味単位の手話タグ列への翻訳モデル構築

### アーキテクチャの共通性

Phase 13-14で構築したTransformerは**そのまま再利用可能**：

- Phase 13-14: `文字列 → [Transformer] → 次文字確率分布`
- Phase 15: `文字列 → [Transformer] → タグ確率分布`

**変更点は語彙定義のみ**：

- `vocab_size = 100`（ひらがな） → `vocab_size = N`（手話タグ）
- Embedding、Attention、Feed-Forward層は完全共通
- 訓練ループもそのまま流用

### タグセット仕様（平坦なリスト）

**タグの形式**: シンプルな意味ラベル（カテゴリー分類なし）

<朝>, <昼>, <夜>,
<挨拶>, <感謝>, <謝罪>,
<会う>, <食べる>, <飲む>,
<家>, <学校>,
<私>, <あなた>,
<綺麗>, <悲しい>,
<はい>, <いいえ>, ...

**初期規模**: 50-100個

**ドキュメンテーション**: `docs/jsl_tagset.md` で各タグの意味と用法例を記載。
実装側ではカテゴリー情報は不要。

### 訓練データスキーマ

**フォーマット**:

```xml

日本語文 <タグ1> <タグ2> ... <タグN>
```

**具体例**:

```xml

おはようございます <朝> <挨拶>
正午に会いましょう <昼> <会う>
私は悲しいです <私> <悲しい>
はい <はい>

```

**重要な特性**:

- **順序の意味性**: 手話は順序が情報を持つ → モデルはタグの順序を正確に学習する必要がある
- **可変長出力**: 1-5個のタグが出現
- **タグの重複**: 「会う 会う」など同じタグが複数回出現する可能性がある
- **パディング**: 最大5個に統一し、不足分は`<PAD>`で埋める
- **複数の妥当な順序**: 違和感判定で、最も自然に感じる順序を1つ選んで記載

### 実装ステップ（Week-by-Week）

#### ステップ1: タグセット設計（Week 1）

- [ ] 手話翻訳に必要な意味単位を50-100個リストアップ
- [ ] 各タグの説明・例文をドキュメント化（`docs/jsl_tagset.md`）

**出力物**: `docs/jsl_tagset.md`（タグ一覧 + 説明 + 例文）

#### ステップ2: 訓練データ作成（Week 2-3）

- [ ] 初期訓練データの手入力作成（100-200文）
  - ファイル: `data/training_data_jsl.txt`
  - フォーマット: 1行1サンプル、`日本語文\t<タグ1> <タグ2> ... <タグN>`
  - **短い文から開始** → 手話の順序特性が学びやすい
- [ ] タグ付与の一貫性確認
  - 数日置いて同じデータを再度チェック
  - ステップ1で定義したルールに従っているか確認

**チェックリスト**:

- [ ] 各行のタグ数は1-5か？
- [ ] タグの順序は違和感判定で選定されているか？
- [ ] タグセット仕様に違反していないか？

**出力物**: `data/training_data_jsl.txt`（100-200サンプル）

#### ステップ3: モデル適応（Week 4）

- [ ] `jsl_vocabulary.rs` 実装
  - タグセット（50-100個）を定数として定義
  - `<PAD>` トークンも語彙に含める
- [ ] `jsl_data.rs` 実装
  - `training_data_jsl.txt` を読み込み
  - 可変長タグ列 → 最大5個でパディング処理
  - バッチイテレーター実装（Phase 14と同様）
- [ ] `config.rs` に JSL 用設定追加
  - `JSL_VOCAB_SIZE`、`JSL_SEQ_LEN=5` など

#### ステップ4: 訓練・推論実装（Week 5）

- [ ] 訓練ループ実行
  - 50-100エポックで試験訓練
  - 損失曲線を監視（低下トレンドの確認）
- [ ] テキスト生成（推論）機能
  - 入力: 日本語テキスト
  - 出力: 予測タグ列（`<タグ1> <タグ2> ...`）

#### ステップ5: 定性評価（Week 6）

- [ ] テスト用データの準備（訓練に使用していない20-30文）
- [ ] 生成されたタグの手動・目視チェック
  - 「生成されたタグは入力日本語の意味を正しく表現しているか？」
  - 「タグの順序は自然か？」
  - 「不要なタグはないか？」
- [ ] 改善が必要な領域を特定・記録

**出力物**: 評価レポート（定性的フィードバック）

### プロジェクト構成（Phase 15想定）

```

transformer_burn/
├── src/
│ ├── main.rs # タスク切り替え（"character" or "jsl"）
│ ├── model.rs # 共通Transformerモデル
│ ├── config.rs # タスク別設定
│ ├── vocabulary.rs # 文字用語彙
│ ├── jsl_vocabulary.rs # タグ用語彙（新規）
│ ├── data.rs # 文字予測用データ
│ └── jsl_data.rs # JSL翻訳用データ（新規）
├── data/
│ ├── training_data.txt # 既存（文字予測）
│ └── training_data_jsl.txt # 新規（JSL翻訳、100-200サンプル）
├── docs/
│ └── jsl_tagset.md # タグセット仕様書（新規）
└── results/
└── jsl_evaluation_report.md # 評価レポート（新規）

```

### Phase 15完了時のチェックリスト

- [ ] タグセット設計・ドキュメント作成（50-100個）
- [ ] 訓練データ作成（100-200文、タグ付与完了）
- [ ] モデル適応（jsl_vocabulary.rs、jsl_data.rs実装）
- [ ] 訓練実行（50-100エポック、損失低下を確認）
- [ ] 推論機能実装（日本語→タグ生成）
- [ ] 定性評価完了（目視チェック、改善点の特定）

### 技術的課題と対応

| 課題 | 対応案 | --------------------- |
| **訓練データサイズ不足** | 学習率低下、早期終止、段階的データ拡張検討 |
| **手話の文法的順序を学習できるか** | モデル側では対応不可。訓練データの質が鍵 → タグ順序を違和感判定で厳選 |
| **タグの粒度不適切** | 評価フェーズで修正。タグセットを段階的に拡張 |
| **複数タグ出現時の順序** | 訓練データで順序を明確に定義。モデルが学習 |

### Phase 14完了後の実装予定

- [x] より深いモデル（3-4層）への拡張
- [x] 可変長シーケンス（パディングとマスキング）
- [x] モデル次元の拡大（d_model=16）
- [x] Burn移行（バッチ処理で8.6倍、GPU最適化で15.5倍高速化達成）
- [ ] Phase 14: 1000エポック訓練でPhase 13同等の精度達成
- [ ] Phase 15: 日本語→手話タグ翻訳システム
- [ ] より長いシーケンス（20-50文字）への拡張
- [ ] 大規模モデル（d_model=64-256）への拡張
- [ ] エンコーダー・デコーダー構造（翻訳タスク）

## 技術制約と実装方針

### 実装方針

#### Phase 1-13（attention_test）

**IMPORTANT**: 以下の実装方針を厳守すること

1. **完全手動実装**: ndarrayなどの外部ライブラリは一切使用しない
2. **段階的アプローチ**: 各層を独立して実装し、段階的に統合
3. **検証重視**: 各層の勾配を数値微分と比較して正確性を確認

#### Phase 14（transformer_burn）

**IMPORTANT**: Burnフレームワーク移行の方針

1. **既存ロジック参考**: attention_testの実装を参考に、Tensor演算に変換
2. **段階的移行**: 層ごとに実装して動作確認（Embedding → Attention → FF → Block）
3. **自動微分活用**: Burnの`Autodiff`で手動バックプロパゲーション不要
4. **GPU最適化**: wgpuバックエンドで10-100倍の高速化を目指す

### 技術スタック

#### Phase 1-13（attention_test）

- **言語**: Rust
- **ライブラリ**: なし（Vec、配列、標準ライブラリのみ）
- **実装レベル**: 行列演算を含む全てを手動実装

#### Phase 14（transformer_burn）

- **言語**: Rust
- **フレームワーク**: Burn 0.18.0
- **バックエンド**: Wgpu（GPU）、Autodiff（自動微分）
- **実装レベル**: Burnの高レベルAPIを活用

### 現在の設定（Phase 13時点）

- 語彙サイズ: 100文字（ひらがな全種類 + 記号 + 長音 + PADトークン）
- シーケンス長: 10文字（SEQ_LEN定数で管理、可変長入力に自動対応）
- モデル次元: **d_model=16**（Phase 12から2倍に拡張）
- Multi-Head: 2ヘッド（d_head=8）
- Feed-Forward中間層: d_ff=32
- **Transformerブロック層数: 4層**（NUM_LAYERS定数で管理）
- **学習パラメータ数: 約27,000個**（各層に独立したパラメータ、埋め込みと出力層は共通）
- 訓練データ: 1378パターン（data/training_data.txt、長文25件追加）
- アーキテクチャ: Pre-LN方式、2箇所の残差接続、複数層スタック構造、Attention Mask
- ハイパーパラメータ: learning_rate=0.0005, epochs=1000

## ユーザー背景

- Next.js、TypeScript、Tailwind CSS、Supabaseでの開発経験
- **Rust初心者**: 基本文法は理解しているが、細かい仕様は学習中
- UI/UX設計、社会的価値、ケア・ステークホルダー視点を重視

### 現在の理解レベル

#### Phase 14完了時点

- ✓ Attention機構、Multi-Head Attention、バックプロパゲーションの完全理解
- ✓ Transformerブロック（Attention + Feed-Forward + Layer Norm + 残差接続）
- ✓ 文字レベル自然言語処理、テキスト生成
- ✓ 複数層のスタック構造、逆伝播の連鎖
- ✓ Pre-LN方式、残差接続の威力、深いモデルの表現力
- ✓ パディング・マスキング機構、可変長シーケンス対応
- ✓ モデル容量とデータ複雑性のバランス、ハイパーパラメータ調整
- ✓ Burnフレームワークの基本構造（Module, Backend, Tensor）
- ✓ ジェネリクスによるバックエンド抽象化（`<B: Backend>`）
- ✓ Tensorの形状管理とreshape/slice操作
- ✓ Burnの組み込みレイヤー活用（Embedding, Linear, LayerNorm）
- ✓ カスタムモジュール実装（`#[derive(Module)]`マクロ）
- ⏳ Autodiffバックエンドと自動微分（学習中）
- ⏳ Optimizerとパラメータ更新（学習中）

## コミュニケーション指針

### YOU MUST follow these communication rules:

1. **言語**: 必ず日本語で回答すること

2. **スタイル**:
   - 論理的で簡潔、かつ実践的で実行可能
   - 段階的な説明を重視
   - 形式的だが過度に堅苦しくない

3. **コード指示の出し方**（CRITICAL）:
   - **必ず行数を明示する**: 「XXX行目に追加」「YYY行目を修正」と具体的に指定
   - **Rust初心者を考慮**: タイプ、所有権、トレイトなど、分かりにくい部分は丁寧に説明
   - **修正前後を明示**: 変更箇所が明確に分かるように提示
   - **複数ファイルの場合**: ファイル名と行数を両方明記

   例：

```

src/attention.rs の 215行目の後に追加:

修正前（103行目）:
pub fn create_output_weight() -> Matrix {

修正後:
fn create_output_weight() -> Matrix {

```

4. **コード出力**: ソースコードは出力しない（ユーザーが手書きで実装）

5. **不確実性**: 分からないことや限界は率直に認める

6. **絵文字**: 使用しない

### 対話方針

- アルゴリズムの考え方、実装方針、デバッグのアプローチを説明
- 具体的なコードではなく、疑似コードや概念図で説明
- 行き詰まった際の問題解決をサポート
- 建設的で論理的なフィードバックを提供
- Rust特有の概念（借用、ライフタイム、トレイト）は必要に応じて補足説明

## デバッグ支援

### デバッグ時のアプローチ

**基本戦略**:

1. 中間結果を段階的に出力して確認
2. 小さな入力（2x2行列など）で手計算と比較
3. 各関数を独立してテスト
4. エッジケース（ゼロ行列、単位行列）で動作確認

**学習時の確認項目**:

- 数値微分と解析的勾配を比較（勾配チェック）
- 学習曲線を観察（損失が下がっているか）
- 勾配の符号を確認（正解方向に更新されているか）
- 各層の勾配の大きさのバランス

---

**Note**: このファイルはリビングドキュメントとして、実装の進捗に応じて更新する。

**最終更新**: Phase 14完了、Phase 15計画策定（2025年10月18日）

- Phase 14: Burn移行プロジェクト完了
- 全レイヤー実装完了（モデル、データ、訓練、推論）
- バッチ処理で8.6倍、GPU最適化で15.5倍の高速化達成
- Mac M2 Air（開発）、Windows RTX 3070（訓練）のクロスプラットフォーム環境構築
- 100エポック訓練で学習動作確認（損失2.81、単語境界学習開始）
- Windows PCで1000エポック訓練を検証中
- Phase 15: 日本語→手話タグ翻訳システムの計画策定完了
- Phase 13-14のTransformerアーキテクチャを完全再利用
- 6週間の実装計画（データ準備→モデル適応→訓練・推論）
- 次ステップ: タグセット設計（50-100個）と訓練データ作成

```

```

```

```
