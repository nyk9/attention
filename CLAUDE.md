# Transformer/Attention 手動実装プロジェクト

## プロジェクト概要

日本語手話通訳アプリの開発に向けた基礎学習として、TransformerとAttentionメカニズムを完全手動実装（外部ライブラリなし）。

**完了フェーズ**: Phase 1-14（Burn移行、GPU + バッチ処理対応）
**次フェーズ**: Phase 15a（日本語→手話タグ翻訳、デコーダーのみ）

## 現在のステータス（Phase 14完了）

### Phase 14: Burn移行プロジェクト（完了）

**プロジェクト名**: `transformer_burn`
**場所**: `rust/attention/transformer_burn/`
**開始日**: 2025年10月10日

#### 完了した実装

✅ **インフラ・基盤**:

- Cargo.toml設定（Burn 0.18.0 + wgpu backend）
- モジュール構造（config, vocabulary, model, data）
- 訓練データ移行（2879サンプル生成）

✅ **モデル層**:

- Embedding層（`burn::nn::Embedding` + カスタムPositional Encoding）
- Multi-Head Attention（カスタム実装、2ヘッド、d_head=8）
- Feed-Forward層（`burn::nn::Linear` + ReLU、d_ff=32）
- Layer Normalization（`burn::nn::LayerNorm`）
- Transformerブロック（Pre-LN方式、残差接続×2）
- 4層スタック構造（NUM_LAYERS=4）
- 出力射影層（d_model → vocab_size=100）

✅ **データ層**:

- Vocabulary実装（既存ロジック移植、100語彙）
- TrainingDataローダー（training_data.txt読み込み、2879サンプル）
- バッチイテレーター実装（可変バッチサイズ対応）

✅ **訓練・推論層**:

- 訓練ループ（Autodiff + Adam最適化）
- 損失関数（CrossEntropyLoss、パディングトークン除外）
- テキスト生成機能（greedy sampling）
- バッチ処理対応（バッチサイズ32/128で動作確認）

✅ **GPU最適化**:

- Wgpuバックエンド統合（再帰制限256で解決）
- Windows RTX 3070（Vulkan）動作確認
- バッチサイズ最適化（128で最高性能）

#### 技術スタック（Phase 14）

- **フレームワーク**: Burn 0.18.0
- **バックエンド**: Wgpu（GPU、Metal/Vulkan）、NdArray（CPU、比較用）
- **自動微分**: Autodiff
- **最適化**: Adam（beta1=0.9, beta2=0.999, epsilon=1e-8）
- **モデル設定**: Phase 13と同一（d_model=16, 4層, 2ヘッド）
- **バッチサイズ**: 32/128（GPU最適化）

#### 性能比較結果（10エポック）

| バックエンド                    | バッチサイズ      | 実行時間 | 対Phase 13比   |
| ------------------------------- | ----------------- | -------- | -------------- |
| Phase 13 CPU手動実装            | 1（サンプルごと） | 155秒    | 基準           |
| **NdArray (CPU)**               | 32                | **18秒** | **8.6倍高速**  |
| Wgpu (GPU) Windows RTX 3070     | 32                | 37秒     | 4.2倍高速      |
| **Wgpu (GPU) Windows RTX 3070** | **128**           | **10秒** | **15.5倍高速** |

**重要な知見**:

- バッチ処理により8.6倍高速化（CPU）
- GPUはバッチサイズ128で真価発揮（バッチ32では転送オーバーヘッドが支配的）
- 小規模モデルではCPUバッチ処理が最もバランス良い選択肢

#### 訓練結果（100エポック、Windows RTX 3070、バッチ128）

- **訓練時間**: 69秒（Phase 13の1000エポック = 1675秒に対し、100エポックで約1/24の時間）
- **損失**: 4.45 → 2.81
- **生成テキスト**: "こんにちは" → "こんにちはいききます。。。。。。。"
- **学習状況**: 単語境界を学習開始（「いく」「ます」のパターン認識）
- **今後**: 1000エポック訓練で Phase 13 同等の精度（損失0.01、88.7%精度）を目指す

---

## Phase 13のステータス（完了）

- **アーキテクチャ**: 4層Transformerデコーダー（Pre-LN方式）
- **モデルサイズ**: d_model=16, 2ヘッド, d_ff=32
- **次文字予測精度**: 88.7%（6文字入力）
- **損失**: 0.0100（初期5.19から99.8%改善）
- **訓練時間**: 1675秒（約28分、1000エポック）
- **可変長入力**: 任意長→10文字に自動パディング対応

## Phase 15a: 日本語→手話タグ翻訳（デコーダーのみ）

**目標**: 日本語テキスト → 意味単位の手話タグ列への翻訳モデル構築（デコーダーのみ実装）

**期間**: 2-3週間

### アーキテクチャ設計

Phase 13-14で構築した**Transformerデコーダーを最小限の変更で再利用**：

**入出力の変更**：
- Phase 13-14: `文字列の一部 → [Decoder] → 次の1文字`（次トークン予測）
- Phase 15a: `日本語文（完全） → [Decoder] → タグ列（最大5個）`（翻訳）

**データ形式の変更**：
- 訓練データ: `日本語文[TAB]<タグ1> <タグ2> ... <タグN>`
- 入力シーケンス: `[日本語トークン列 + パディング]`（SEQ_LEN=10）
- 出力シーケンス: `[タグ1, タグ2, タグ3, タグ4, タグ5]`（固定5位置）

**モデルの変更**：
- Embedding、Attention、Feed-Forward層: **変更なし**
- 訓練方式: **次トークン予測から、固定位置分類タスクへ変更**
  - 入力の最後の位置（SEQ_LEN-1）で5つのタグを予測
  - 各タグ位置で独立した分類（vocab_sizeクラス分類）

**技術的ポイント**：
- 語彙: 日本語文字（86種） + 手話タグ（80種） = 合計166種
- 出力層: 5位置 × 166クラス分類
- 損失関数: 5位置の CrossEntropyLoss を合計

### タグセット仕様（平坦なリスト）

**タグの形式**: シンプルな意味ラベル（カテゴリー分類なし）

<朝>, <昼>, <夜>,
<挨拶>, <感謝>, <謝罪>,
<会う>, <食べる>, <飲む>,
<家>, <学校>,
<私>, <あなた>,
<綺麗>, <悲しい>,
<はい>, <いいえ>, ...

**初期規模**: 50-100個

**ドキュメンテーション**: `docs/jsl_tagset.md` で各タグの意味と用法例を記載。
実装側ではカテゴリー情報は不要。

### 訓練データスキーマ

**フォーマット**:

```xml

日本語文 <タグ1> <タグ2> ... <タグN>
```

**具体例**:

```xml

おはようございます <朝> <挨拶>
正午に会いましょう <昼> <会う>
私は悲しいです <私> <悲しい>
はい <はい>

```

**重要な特性**:

- **順序の意味性**: 手話は順序が情報を持つ → モデルはタグの順序を正確に学習する必要がある
- **可変長出力**: 1-5個のタグが出現
- **タグの重複**: 「会う 会う」など同じタグが複数回出現する可能性がある
- **パディング**: 最大5個に統一し、不足分は`<PAD>`で埋める
- **複数の妥当な順序**: 違和感判定で、最も自然に感じる順序を1つ選んで記載

### 実装ステップ（Phase 15a）

**前提**: タグセット（80個）と訓練データ（435サンプル）は既に作成済み

#### ステップ1: データローダー修正（Week 1前半）

**現状の問題**:
- `jsl_data.rs`（41-51行目）が Phase 13-14 の「次トークン予測」形式
- 日本語とタグを混ぜた文字列を生成し、各位置で次トークンを予測

**修正内容**:
- [ ] `jsl_data.rs` の `load` 関数を修正
  - 日本語文のみをエンコード → 入力シーケンス（SEQ_LEN=10）
  - タグ列のみをエンコード → 出力シーケンス（固定5位置）
  - サンプル形式: `(Vec<i32>, [i32; 5])`（入力10次元、出力5次元）
- [ ] バッチイテレーター修正
  - 入力: `[batch_size, SEQ_LEN]`
  - 出力: `[batch_size, 5]`（5タグ位置）

#### ステップ2: 訓練ループ修正（Week 1後半）

**修正内容**:
- [ ] `main.rs` の `train_jsl` 関数（36-82行目）を修正
  - 現状: 最後の1位置のみで損失計算（58-64行目）
  - 修正後: 5タグ位置すべてで損失計算
  - 損失: `sum(CrossEntropyLoss(tag_pos_i)) for i in 0..5`
- [ ] 5位置の出力を取得する処理を追加
  - `logits`: `[batch_size, SEQ_LEN, vocab_size]`
  - `logits_last`: `[batch_size, vocab_size]`（最後の位置のみ）
  - → 5回のforward passで5タグ分を予測

#### ステップ3: 推論機能修正（Week 2前半）

**修正内容**:
- [ ] `main.rs` の `predict_tags` 関数（87-118行目）を修正
  - 現状: 1トークンのみ予測
  - 修正後: 5タグ位置すべてを予測
  - 出力形式: `"<タグ1> <タグ2> <PAD> <PAD> <PAD>"`
  - PADトークンは出力から除外

#### ステップ4: 訓練実行・評価（Week 2後半 - Week 3）

- [ ] 100エポック訓練を実行
  - 損失曲線の確認（5位置の合計損失が低下するか）
  - 各タグ位置の損失を個別に監視
- [ ] テスト文での推論確認
  - 「わたしはたべます」→ `<私> <食べる> <PAD> <PAD> <PAD>`
  - 「おはようございます」→ `<朝> <挨拶> <PAD> <PAD> <PAD>`
- [ ] 定性評価
  - 訓練データ外の20-30文でテスト
  - 正解率、タグの妥当性を目視チェック

### プロジェクト構成（Phase 15想定）

```

transformer_burn/
├── src/
│ ├── main.rs # タスク切り替え（"character" or "jsl"）
│ ├── model.rs # 共通Transformerモデル
│ ├── config.rs # タスク別設定
│ ├── vocabulary.rs # 文字用語彙
│ ├── jsl_vocabulary.rs # タグ用語彙（新規）
│ ├── data.rs # 文字予測用データ
│ └── jsl_data.rs # JSL翻訳用データ（新規）
├── data/
│ ├── training_data.txt # 既存（文字予測）
│ └── training_data_jsl.txt # 新規（JSL翻訳、100-200サンプル）
├── docs/
│ └── jsl_tagset.md # タグセット仕様書（新規）
└── results/
└── jsl_evaluation_report.md # 評価レポート（新規）

```

### Phase 15a 完了時のチェックリスト

- [ ] データローダー修正（jsl_data.rs）
  - [ ] 日本語とタグを分離してエンコード
  - [ ] 固定5タグ出力形式への変更
- [ ] 訓練ループ修正（main.rs の train_jsl）
  - [ ] 5タグ位置すべてで損失計算
- [ ] 推論機能修正（main.rs の predict_tags）
  - [ ] 5タグ予測への変更
  - [ ] PADトークン除外処理
- [ ] 訓練実行（100エポック）
  - [ ] 損失低下の確認
- [ ] 定性評価完了
  - [ ] テスト文で正しいタグが生成されるか確認
  - [ ] 改善点の特定

### 技術的課題と対応（Phase 15a）

| 課題 | 対応案 |
| --------------------- | --------------------- |
| **訓練データサイズ不足**（435サンプル） | 学習率調整、早期終止、データ拡張検討 |
| **固定5位置出力の制約** | 1-5個のタグを固定5位置に配置、不足分はPADで埋める |
| **タグ順序の学習精度** | 訓練データの質が鍵。違和感判定で順序を厳選 |
| **5タグを超える文への対応** | Phase 15aでは対応不可。Phase 15bでSeq2Seq実装を検討 |

---

## Phase 15b: Seq2Seqへの拡張（オプション、計画）

**目標**: より長いタグ列、複雑な文に対応するため、エンコーダー・デコーダー構造へ拡張

**期間**: 5-6週間

**実装内容**:
- エンコーダー層の実装（Self-Attention + Feed-Forward）
- Cross-Attention層の実装（デコーダーがエンコーダー出力を参照）
- デコーダー修正（Self-Attention + Cross-Attention + Feed-Forward）
- 可変長出力対応（最大長20-30タグ）
- ビームサーチによる推論品質向上

**Phase 15aからの変更点**:
- 入力と出力を完全分離（エンコーダー/デコーダー）
- 出力長の制約を緩和（固定5 → 可変20-30）
- より複雑な文への対応

**判断基準**:
- Phase 15aの評価結果を見て実装要否を判断
- 固定5タグで十分な精度が出れば、Phase 15bは不要

### Phase 14完了後の実装予定

- [x] より深いモデル（3-4層）への拡張
- [x] 可変長シーケンス（パディングとマスキング）
- [x] モデル次元の拡大（d_model=16）
- [x] Burn移行（バッチ処理で8.6倍、GPU最適化で15.5倍高速化達成）
- [ ] Phase 14: 1000エポック訓練でPhase 13同等の精度達成
- [ ] **Phase 15a**: 日本語→手話タグ翻訳（デコーダーのみ、2-3週間）
- [ ] Phase 15b: Seq2Seqへの拡張（オプション、5-6週間）
- [ ] より長いシーケンス（20-50文字）への拡張
- [ ] 大規模モデル（d_model=64-256）への拡張

## 技術制約と実装方針

### 実装方針

#### Phase 1-13（attention_test）

**IMPORTANT**: 以下の実装方針を厳守すること

1. **完全手動実装**: ndarrayなどの外部ライブラリは一切使用しない
2. **段階的アプローチ**: 各層を独立して実装し、段階的に統合
3. **検証重視**: 各層の勾配を数値微分と比較して正確性を確認

#### Phase 14（transformer_burn）

**IMPORTANT**: Burnフレームワーク移行の方針

1. **既存ロジック参考**: attention_testの実装を参考に、Tensor演算に変換
2. **段階的移行**: 層ごとに実装して動作確認（Embedding → Attention → FF → Block）
3. **自動微分活用**: Burnの`Autodiff`で手動バックプロパゲーション不要
4. **GPU最適化**: wgpuバックエンドで10-100倍の高速化を目指す

### 技術スタック

#### Phase 1-13（attention_test）

- **言語**: Rust
- **ライブラリ**: なし（Vec、配列、標準ライブラリのみ）
- **実装レベル**: 行列演算を含む全てを手動実装

#### Phase 14（transformer_burn）

- **言語**: Rust
- **フレームワーク**: Burn 0.18.0
- **バックエンド**: Wgpu（GPU）、Autodiff（自動微分）
- **実装レベル**: Burnの高レベルAPIを活用

### 現在の設定（Phase 13時点）

- 語彙サイズ: 100文字（ひらがな全種類 + 記号 + 長音 + PADトークン）
- シーケンス長: 10文字（SEQ_LEN定数で管理、可変長入力に自動対応）
- モデル次元: **d_model=16**（Phase 12から2倍に拡張）
- Multi-Head: 2ヘッド（d_head=8）
- Feed-Forward中間層: d_ff=32
- **Transformerブロック層数: 4層**（NUM_LAYERS定数で管理）
- **学習パラメータ数: 約27,000個**（各層に独立したパラメータ、埋め込みと出力層は共通）
- 訓練データ: 1378パターン（data/training_data.txt、長文25件追加）
- アーキテクチャ: Pre-LN方式、2箇所の残差接続、複数層スタック構造、Attention Mask
- ハイパーパラメータ: learning_rate=0.0005, epochs=1000

## ユーザー背景

- Next.js、TypeScript、Tailwind CSS、Supabaseでの開発経験
- **Rust初心者**: 基本文法は理解しているが、細かい仕様は学習中
- UI/UX設計、社会的価値、ケア・ステークホルダー視点を重視

### 現在の理解レベル

#### Phase 14完了時点

- ✓ Attention機構、Multi-Head Attention、バックプロパゲーションの完全理解
- ✓ Transformerブロック（Attention + Feed-Forward + Layer Norm + 残差接続）
- ✓ 文字レベル自然言語処理、テキスト生成
- ✓ 複数層のスタック構造、逆伝播の連鎖
- ✓ Pre-LN方式、残差接続の威力、深いモデルの表現力
- ✓ パディング・マスキング機構、可変長シーケンス対応
- ✓ モデル容量とデータ複雑性のバランス、ハイパーパラメータ調整
- ✓ Burnフレームワークの基本構造（Module, Backend, Tensor）
- ✓ ジェネリクスによるバックエンド抽象化（`<B: Backend>`）
- ✓ Tensorの形状管理とreshape/slice操作
- ✓ Burnの組み込みレイヤー活用（Embedding, Linear, LayerNorm）
- ✓ カスタムモジュール実装（`#[derive(Module)]`マクロ）
- ⏳ Autodiffバックエンドと自動微分（学習中）
- ⏳ Optimizerとパラメータ更新（学習中）

## コミュニケーション指針

### YOU MUST follow these communication rules:

1. **言語**: 必ず日本語で回答すること

2. **スタイル**:
   - 論理的で簡潔、かつ実践的で実行可能
   - 段階的な説明を重視
   - 形式的だが過度に堅苦しくない

3. **コード指示の出し方**（CRITICAL）:
   - **必ず行数を明示する**: 「XXX行目に追加」「YYY行目を修正」と具体的に指定
   - **Rust初心者を考慮**: タイプ、所有権、トレイトなど、分かりにくい部分は丁寧に説明
   - **修正前後を明示**: 変更箇所が明確に分かるように提示
   - **複数ファイルの場合**: ファイル名と行数を両方明記

   例：

```

src/attention.rs の 215行目の後に追加:

修正前（103行目）:
pub fn create_output_weight() -> Matrix {

修正後:
fn create_output_weight() -> Matrix {

```

4. **コード出力**: ソースコードは出力しない（ユーザーが手書きで実装）

5. **不確実性**: 分からないことや限界は率直に認める

6. **絵文字**: 使用しない

### 対話方針

- アルゴリズムの考え方、実装方針、デバッグのアプローチを説明
- 具体的なコードではなく、疑似コードや概念図で説明
- 行き詰まった際の問題解決をサポート
- 建設的で論理的なフィードバックを提供
- Rust特有の概念（借用、ライフタイム、トレイト）は必要に応じて補足説明

## デバッグ支援

### デバッグ時のアプローチ

**基本戦略**:

1. 中間結果を段階的に出力して確認
2. 小さな入力（2x2行列など）で手計算と比較
3. 各関数を独立してテスト
4. エッジケース（ゼロ行列、単位行列）で動作確認

**学習時の確認項目**:

- 数値微分と解析的勾配を比較（勾配チェック）
- 学習曲線を観察（損失が下がっているか）
- 勾配の符号を確認（正解方向に更新されているか）
- 各層の勾配の大きさのバランス

---

**Note**: このファイルはリビングドキュメントとして、実装の進捗に応じて更新する。

**最終更新**: Phase 15を Phase 15a/15b に分割（2025年10月19日）

- Phase 14: Burn移行プロジェクト完了
- 全レイヤー実装完了（モデル、データ、訓練、推論）
- バッチ処理で8.6倍、GPU最適化で15.5倍の高速化達成
- Mac M2 Air（開発）、Windows RTX 3070（訓練）のクロスプラットフォーム環境構築
- 100エポック訓練で学習動作確認（損失2.81、単語境界学習開始）
- **Phase 15a**: 日本語→手話タグ翻訳（デコーダーのみ、2-3週間計画）
  - タグセット80個、訓練データ435サンプル作成済み
  - 現在の問題を特定：データローダーが「次トークン予測」形式
  - 修正方針：固定5タグ位置での分類タスクへ変更
- **Phase 15b**: Seq2Seqへの拡張（オプション、5-6週間計画）
  - Phase 15aの評価結果を見て実装要否を判断
- 次ステップ: データローダー修正（jsl_data.rs）

```

```

```

```
