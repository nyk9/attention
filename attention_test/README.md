# Transformer/Attention 手動実装プロジェクト

完全手動実装による日本語テキスト生成Transformerモデル（Rust）

## プロジェクト概要

日本語手話通訳アプリの開発に向けた基礎学習として、TransformerとAttentionメカニズムを**外部ライブラリなし**で完全手動実装しました。行列演算からバックプロパゲーションまで、すべてをVecと標準ライブラリのみで実装しています。

## 現在の成果（Phase 13完了）

- **アーキテクチャ**: 4層Transformerデコーダー（Pre-LN方式 + Attention Mask）
- **モデルサイズ**: d_model=16, 2ヘッド, d_ff=32
- **学習パラメータ数**: 約27,000個
- **次文字予測精度**: 88.7%（6文字入力でも高精度）
- **損失**: 0.0100（初期5.19から99.8%改善）
- **訓練時間**: 1675秒（約28分、1000エポック）
- **語彙サイズ**: 100文字（ひらがな全種類 + 記号 + PADトークン）
- **シーケンス長**: 10文字（可変長入力に自動対応）

### 動作例

```
入力: "ともだちとべ"（6文字）
パディング後: [19, 34, 56, 16, 19, 64, 99, 99, 99, 99]
次文字予測: "ん" 88.7%（ともだちとべんきょう）
```

## 技術スタック

- **言語**: Rust
- **ライブラリ**: なし（Vec、配列、標準ライブラリのみ）
- **実装レベル**: 行列演算を含む全てを手動実装

## アーキテクチャ

```
入力文字列
    ↓
埋め込み層 + Position Encoding
    ↓
┌─────────────────────────┐
│ Transformer Block 1     │
│  ├ Layer Norm           │
│  ├ Multi-Head Attention │
│  ├ Residual Connection  │
│  ├ Layer Norm           │
│  ├ Feed-Forward         │
│  └ Residual Connection  │
└─────────────────────────┘
    ↓
┌─────────────────────────┐
│ Transformer Block 2     │
│  ├ Layer Norm           │
│  ├ Multi-Head Attention │
│  ├ Residual Connection  │
│  ├ Layer Norm           │
│  ├ Feed-Forward         │
│  └ Residual Connection  │
└─────────────────────────┘
    ↓
出力層（Softmax）
    ↓
次文字予測
```

## 実装済みコンポーネント

### 基礎演算
- [x] 行列演算（積、転置、加算、減算、Hadamard積）
- [x] Softmax、ReLU活性化関数
- [x] Position Encoding

### Attention機構
- [x] Self-Attention（Query, Key, Value）
- [x] Attention Scores計算
- [x] Multi-Head Attention（2ヘッド、d_head=4）
- [x] Scaled Dot-Product Attention

### Transformerブロック
- [x] Feed-Forward層（2層構造、ReLU活性化）
- [x] Layer Normalization（順伝播・逆伝播）
- [x] 残差接続（Residual Connection）
- [x] Pre-LN方式の実装
- [x] 複数層のスタック構造（2層）

### 学習機能
- [x] Cross-Entropy Loss
- [x] バックプロパゲーション（全27個のパラメータ）
- [x] 勾配降下法による訓練ループ
- [x] 複数層の逆伝播

### 自然言語処理
- [x] 語彙管理（Vocabulary構造体）
- [x] トークナイザー（encode/decode）
- [x] 文字列の埋め込み
- [x] 次トークン予測
- [x] テキスト生成（自己回帰的生成）
- [x] 文末トークン停止機能
- [x] パディング機構（可変長→固定長変換）
- [x] Attention Mask（PAD位置を無視）

## Phase別実装履歴

### Phase 1-4: 基礎構築
- Self-Attention機構の実装
- 次トークン予測
- Position Encoding

### Phase 5: 学習機能
- 損失関数（Cross-Entropy Loss）
- 勾配降下法
- 訓練ループ

### Phase 6: バックプロパゲーション
- 出力層の逆伝播
- 解析的勾配計算

### Phase 7a: 全層バックプロパゲーション
- 予測精度: 79.3% → 99.6%
- 全層学習の劇的な効果を確認

### Phase 7b: Multi-Head Attention
- 2ヘッド構造の実装
- 予測精度: 99.8%達成

### Phase 8: Feed-Forward層
- 完全なTransformerブロック実装
- ReLU活性化による非線形変換
- 予測精度: 98.9%

### Phase 9: 文字レベル自然言語処理
- 語彙サイズ: 10トークン → 100文字
- 日本語テキスト生成を実現
- 次文字予測精度: 100%
- 損失: 6.71 → 0.0855（98.7%改善）
- 訓練時間: 1.28秒（500エポック）

### Phase 10: 長文対応
- シーケンス長: 3文字 → 10文字
- SEQ_LEN定数の導入
- 訓練データの外部ファイル化
- 訓練データ数: 19 → 77パターン
- 次文字予測精度: 99.6%
- 損失: 5.84 → 0.07（98.9%改善）
- 訓練時間: 15.71秒（1000エポック）

### Phase 11: Layer Normalizationと残差接続
- Pre-LN方式の実装
- 残差接続（2箇所）
- γ・βパラメータの学習
- 学習パラメータ数: 11個 → 15個
- 次文字予測精度: 98.6%
- 損失: 4.53 → 0.0006（99.99%改善）
- 訓練時間: 57.59秒（1000エポック）
- 訓練データ数: 247パターン

### Phase 12: 複数Transformerブロックの積み重ね
- NUM_LAYERS定数の導入（2層）
- 複数層の順伝播・逆伝播
- スタック構造の実装
- 各層の独立学習
- 学習パラメータ数: 15個 → 約27個
- 次文字予測精度: 94.6%
- 損失: 0.0006 → 0.0003（50%改善）
- 訓練時間: 57.59秒 → 104.38秒（1.8倍増加）

### Phase 13: モデル拡大と可変長入力対応
- **4層への拡張**: NUM_LAYERS=4、深いモデルの表現力向上
- **d_model=16**: モデル次元2倍化でパラメータ数約4倍増
- **パディング機構**: `pad_sequence`関数で任意長→10文字に自動変換
- **Attention Mask**: PAD位置を-∞でマスクし、Softmax後に確率0に
- **長文訓練データ**: 40-60文字の複文25件追加（訓練データ1378パターン）
- **ハイパーパラメータ調整**: learning_rate=0.0005に削減（データ複雑化対応）
- 学習パラメータ数: 約27個 → 約27,000個
- 次文字予測精度: 88.7%（6文字入力）
- 損失: 0.0100（初期5.19から99.8%改善）
- 訓練時間: 1675秒（約28分、1000エポック）

## 重要な学習知見

### バックプロパゲーションの威力
- 数値微分: O(n²)、遅い
- バックプロパゲーション: O(n)、数百倍高速
- 連鎖律による効率的な勾配計算

### 全層学習の効果
- 出力層のみ学習: 79.3%
- 全層同時学習: 99.6%
- 各層が協調して最適化される

### Multi-Head Attentionの本質
- 各ヘッドが独立して異なるパターンを学習
- 複数の視点での並列処理
- 表現力の向上

### 残差接続の重要性
- 勾配の流れを改善（2つの経路に分岐）
- 深い層でも勾配消失を防止
- より深いモデルを可能に

### Layer Normalizationの役割
- 学習の安定化と収束の高速化
- Pre-LN方式がPost-LNより安定
- γ・βパラメータで表現力を維持

### 複数層の威力（Phase 12）
- 各層が異なる抽象度の特徴を学習
  - 第1層: 基本パターン（文字の並び）
  - 第2層: 高次パターン（文末の規則）
- 損失: 0.0006 → 0.0003（50%改善）
- 訓練時間は層数に対して効率的（1.8倍増加）

### モデル容量と表現力（Phase 13）
- **d_model拡大の劇的効果**: 8→16で複雑な長文パターンを学習可能に
  - d_model=8: 損失1.67（収束せず）
  - d_model=16: 損失0.01（99.4%改善）
- **Attention Maskの重要性**: PADトークンを無視し、実質的な文脈のみで予測
- **4層の深さ**: 予測確信度99.9%（2層では94.6%）
- **ハイパーパラメータの最適化**: データ複雑性に応じた学習率調整が重要

### ハイパーパラメータの関係
- シーケンス長と学習率は反比例
  - 3文字: learning_rate = 0.01
  - 10文字: learning_rate = 0.002
- 訓練データ量の指針
  - シーケンス長Nに対して、文章長N+5以上の文章が20個以上必要
- モデル容量とデータ複雑性の関係
  - 訓練データ5倍増 → d_model 2倍増が必要
  - d_model小さすぎ → 破滅的忘却（新パターン学習で旧パターン忘却）

## 訓練データ

### データソース
`data/training_data.txt`（外部ファイル化）

### 訓練データ生成
窓スライド方式で10文字→次1文字の予測タスクを生成
- 例: "このほんはとてもおもしろいです"
  - ['こ','の','ほ',...,'お','も'] → 'し'
  - ['の','ほ','ん',...,'も','し'] → 'ろ'
  - ...
- 合計: 1378パターン（Phase 13で長文データ追加後）

### 日本語文章（例）
```
こんにちは
ありがとうございます
おはようございます
さようなら
おやすみなさい
いただきます
ごちそうさまでした
すみません
よろしくおねがいします
がんばってください
おめでとうございます
おつかれさまです
...（全25文章以上）
```

## 使い方

### ビルド
```bash
cargo build --release
```

### 実行
```bash
cargo run --release
# 入力プロンプトで種文字列を入力（任意の長さ）
ともだちとべ
```

### 出力例
```
=== Phase 9: 文字レベル自然言語処理  ===
語彙サイズ: 51文字（ひらがな46 +  記号5）

入力文字列: "ともだちとべ"
トークンID: [19, 34, 56, 16, 19, 64, 99, 99, 99, 99]

【学習前の予測】
次文字予測（確率上位5つ）:
  '?': 16.4%
  'よ': 16.4%
  ...

=== Transformer学習開始 ===
訓練データ数: 1378

訓練中...
Epoch 100: 平均損失 = 2.1192
Epoch 200: 平均損失 = 1.0946
...
Epoch 1000: 平均損失 = 0.0100

【学習後の予測】
次文字予測（確率上位5つ）:
  'ん': 88.7%
  'し': 6.9%
  ...

=== 学習結果 ===
訓練時間: 1675.41秒
初期損失: 5.1934
最終損失: 0.0100
```

## 今後の展望

### Phase 14以降の拡張案
- Burn移行（GPU活用で10-100倍高速化）
- より長いシーケンス（20-50文字）への対応
- バイアス項の追加
- モデル次元のさらなる拡大（d_model=32-64）

### 長期目標
- 手話⇄日本語翻訳AIの開発
- Encoder-Decoderアーキテクチャ
- Candle or Burnへの移行（GPU活用）
- モバイル版手話通訳アプリ

## ライセンス

このプロジェクトは学習目的のため、自由に利用・改変できます。

## 謝辞

このプロジェクトは、Transformerの仕組みを深く理解するために、外部ライブラリに頼らず完全手動実装を行いました。Phase 1からPhase 13まで段階的に機能を追加し、最終的に可変長入力対応の実用的な日本語テキスト生成モデルを実現できました。

---

**最終更新**: 2025年10月9日（Phase 13完了）
