   Compiling cubecl-common v0.6.0
   Compiling naga v25.0.1
   Compiling wgpu-types v25.0.0
   Compiling rayon v1.11.0
   Compiling darling_core v0.20.11
   Compiling matrixmultiply v0.3.10
   Compiling rmp v0.8.14
   Compiling macerator v0.2.9
   Compiling num-integer v0.1.46
   Compiling burn-ndarray v0.18.0
   Compiling moddef v0.3.0
   Compiling cubecl-ir v0.6.0
   Compiling rmp-serde v1.3.0
   Compiling itertools v0.14.0
   Compiling seq-macro v0.3.6
   Compiling atomic_float v1.1.0
   Compiling burn-common v0.18.0
   Compiling darling_macro v0.20.11
   Compiling ndarray v0.16.1
   Compiling darling v0.20.11
   Compiling macerator-macros v0.1.3
   Compiling cubecl-runtime v0.6.0
   Compiling cubecl-core v0.6.0
   Compiling cubecl-std v0.6.0
   Compiling cubecl-reduce v0.6.0
   Compiling cubecl-random v0.6.0
   Compiling wgpu-hal v25.0.2
   Compiling cubecl-matmul v0.6.0
   Compiling wgpu-core-deps-apple v25.0.0
   Compiling wgpu-core v25.0.2
   Compiling cubecl-convolution v0.6.0
   Compiling wgpu v25.0.2
   Compiling cubecl-wgpu v0.6.0
   Compiling cubecl v0.6.0
   Compiling burn-tensor v0.18.0
   Compiling burn-ir v0.18.0
   Compiling burn-autodiff v0.18.0
   Compiling burn-core v0.18.0
   Compiling burn-fusion v0.18.0
   Compiling burn-cubecl-fusion v0.18.0
   Compiling burn-cubecl v0.18.0
   Compiling burn-wgpu v0.18.0
   Compiling burn v0.18.0
   Compiling transformer_burn v0.1.0 (/Users/kagimotoeiji/Documents/rust/attention/transformer_burn)
warning: unused variable: `target_tensor`
  --> src/main.rs:51:17
   |
51 |             let target_tensor =
   |                 ^^^^^^^^^^^^^ help: if this is intentional, prefix it with an underscore: `_target_tensor`
   |
   = note: `#[warn(unused_variables)]` on by default

warning: unused variable: `logits`
  --> src/main.rs:55:17
   |
55 |             let logits = model.forward(input_tensor);
   |                 ^^^^^^ help: if this is intentional, prefix it with an underscore: `_logits`

warning: type alias `TrainingBackend` is never used
  --> src/main.rs:20:6
   |
20 | type TrainingBackend = Autodiff<NdArray>;
   |      ^^^^^^^^^^^^^^^
   |
   = note: `#[warn(dead_code)]` on by default

warning: function `train` is never used
  --> src/main.rs:22:4
   |
22 | fn train(
   |    ^^^^^

warning: constant `LEARNING_RATE` is never used
  --> src/config.rs:12:11
   |
12 | pub const LEARNING_RATE: f64 = 0.0005; // 学習率
   |           ^^^^^^^^^^^^^

warning: constant `EPOCHS` is never used
  --> src/config.rs:13:11
   |
13 | pub const EPOCHS: usize = 1000; // エポック数
   |           ^^^^^^

warning: fields `id_to_char` and `vocab_size` are never read
 --> src/vocabulary.rs:6:9
  |
4 | pub struct Vocabulary {
  |            ---------- fields in this struct
5 |     pub char_to_id: HashMap<char, usize>,
6 |     pub id_to_char: Vec<char>,
  |         ^^^^^^^^^^
7 |     pub vocab_size: usize,
  |         ^^^^^^^^^^

warning: methods `decode` and `decode_predictions` are never used
   --> src/vocabulary.rs:95:12
    |
10  | impl Vocabulary {
    | --------------- methods in this implementation
...
95  |     pub fn decode(&self, token_ids: &Vec<i32>) -> String {
    |            ^^^^^^
...
111 |     pub fn decode_predictions(&self, predictions: &Vec<(i32, f64)>) -> Vec<(char, f64)> {
    |            ^^^^^^^^^^^^^^^^^^

warning: `transformer_burn` (bin "transformer_burn") generated 8 warnings
    Finished `release` profile [optimized] target(s) in 2m 31s
     Running `target/release/transformer_burn`
入力: こんにちは
トークン: [9, 45, 21, 16, 25, 99, 99, 99, 99, 99]
Embedding shape: Shape { dims: [1, 10, 100] }
Embedding with Positional Encoding 完了
最初のトークンの出力ロジット（最初の10語彙）:
Tensor { primitive: Float({ id: TensorId { value: 444 }, shape: [1, 1, 10], device: DefaultDevice }) }

Transformerの出力形状: [1, 10, 100]
期待される形状: [batch_size=1, seq_len=10, vocab_size=100]
訓練サンプル数: 2879
訓練データ準備完了: 2879サンプル
実行時間: 0.156435875s
